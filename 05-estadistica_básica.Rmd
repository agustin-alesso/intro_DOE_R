# Revisión de estadística básica

En este capítulo revisaremos algunos conceptos y términos estadísticos básicos que necesitaremos para el entender el diseño y análisis estadístico de datos provenientes de estudios observacionales y experimentales. Revisaremos:

- Concepto y utilidad de la Estadística
- Tipos de datos y variables
- Población y muestra
- Estadística descriptiva
- Distribuciones de probabilidades
- Inferencia estadística

## ¿Qué es la estadística?

Existen muchas definiciones de _Estadística_, tantas como libros consultemos. Según Ott y Longnecker (2016), la estadística es la ciencia del diseño de estudios o experimentos, recolección de datos y modelado/análisis de los mismos para la toma de decisiones o descubrimiento de nuevo conocimiento cuando la información disponible es limitada y variable. En resumen, **la estadística es la ciencia del aprendizaje a partir de los datos** (_learning from data_) y está muy emparentada con la aplicación del **método científico**. 

Según este enfoque, el proceso de aprender a partir de los datos implica las siguientes etapas: 
1. **Definición del problema de estudio.** Todo estudio o investigación surge de una pregunta de investigación, un interrogante sobre nuestro objeto de estudio que no ha sido respondido aún con la información existente. Esto motiva la búsqueda de más información (nuevos experimentos o muestreos) para verificar las posibles respuestas o modelos que nos permitan representarlo (hipótesis). Entonces, a partir de la identificación del problema o pregunta de estudio ponemos de relieve cuáles son las variables que intervienen en el proceso, cuáles hay que controlar y cuáles hay que alterar para ver la respuesta, cuál es la información que debemos recolectar, que tipo de datos se van a trabajar, etc. 

2. **Recolección de los datos.** Una vez definido el problema de estudio, necesitamos determinar de qué manera se colectará la información relevante. Para ello debemos diseñar muestreos o experimentos que permitan de manera costo efectiva obtener la mayor cantidad de información empleando el menor tiempo y dinero para lograr responder de manera precisa la pregunta que originó el estudio. La estadísitca brinda herramientas para el esto.

3. **Resumen de la información.** con los datos en mano debemos organizadorlos y resumirlos mediante técnicas núméricas o gráficas para facilitar su exploración y el reconocimiento de las principales características o patrones de los mismos (tendencias, variabilidad, anomalías, etc.). La _estadística descriptiva_ brinda las herramientas necesarias para la descripción de la información proveniente de muestreos o experimentos.

4. **Análisis, interpretación y comunicación de los resultados.** Los datos obtenidos son una pequeña parte (_muestra_) de un conjunto más grande (_población_) el cual es imposible observar en su totalidad. Debemos analizar la información de la muestra para realizar generalizaciones y poder interpretar los resultados en la población. La _estadística inferencial_ brinda herramientas para estimar y valorar los modelos definidos en la Etapa 1 a la luz de los datos recolectados en la Etapa 2 considerando la variabilidad del proceso que los generó. 

## Variables

Las variables son características de interés que se observan o miden en la unidad de observación más pequeña. Al contrario que las constantes, las variables toman diferentes valores de una individuo a otro, i.e. varían. Podemos clasificar las variables según el tipo de datos e información que contienen.

### Tipos de datos

Según el tipo de datos, las variables son:

- *Cualitativas*: que expresan una **cualidad** o atributo no numéricos. _e.g. color de pelo, sexo, estatus sanitario, estado fenológico._
- *Cuantitativas*: que expresan una **cantidad discreta** (_e.g. número de ramas, número de insectos_) o **contínua** (_e.g. peso de granos, contenido de MO del suelo_).

### Escala de medición

Según la cantidad de información que contienen (de menor a mayor) las variables se clasifican en:

- **Nominal**: cualitativa (_el número de identificación o RP de una vaca_)
- **Ordinal**: cualitativa con orden (_posición en el ranking del control lechero_)
- **De intervalo**: cuantitativa, orden y distancias (_fecha del último parto_)
- **De razón**: cuantitativa, orden, distancia y proporciones (_días desde el último parto_)

La escala de medición determina la cantidad de información que tienen y qué métodos podemos aplicar. Siempre podemos analizar variables más completas con técnicas para variables más simples. No obstante, este procedimiento implica pérdida información que puede ser relevante para responder la pregunta de investigación. Por ejemplo, una variable numérica de intervalo o razón podemos convertirla en nominal u ordinal generando intervalos de clases y contabilizar las frecuencias de cada clase. Por el contrario, las variables más simples no pueden ser analizadas con técnicas diseñadas para variables más completas. Ejemplo: no podemos obtener el promedio de una variable _nominal_ como el color de pelo.

## Población y muestra

Desde el punto de vista estadístico, una **Población** es la totalidad de las unidades u observaciones individuales sobre la cuales queremos _realizar la inferencia_. Está _definida en el tiempo y espacio_ y se caracteriza por sus _parámetros_, i.e. la media $\mu$. Las poblaciones pueden ser finitas, i.e. podemos contar la totalidad de elementos que contienen, o infinita, i.e. no los podemos contar. En la mayoría de los casos, los métodos estadísticos asumen que la población es infinita o finitas pero tan grandes que a los efectos prácticos puede asumirse que son infinitas.

Por su parte, una **muestra** es un subconjunto de individuos u observaciones individuales que elegimos de la población. Las muestras son finitas y se caracterizan por sus _estadísticos_  que son la versión muestral de los parámetros poblacionales, i.e. la media muestral $\bar{y}$. Si el procedimiento que utilizamos para obtener la muestra es aleatorio entonces podemos asumir que la muestra tendrá las mismas propiedades que la población y por lo tanto podemos usar dicha información para inferencia características de la poblaicón. En el siguiente gráfico resume el concepto general de la inferencia estadística. 

```{r, fig.cap="Esquema del proceso de inferencia estadística", out.width="80%", echo = F}
include_graphics("images/poblacion_muestra_venn.svg",  auto_pdf = T)
```

Supongamos que queremos conocer el valor medio de las alturas (_parámetro_) en m de plantas de maíz de un lote de 50 has, i.e. $\mu = ?$. El conjunto más grande denominado _Población_ contiene las alturas en m de todas las plantas de maíz del lote en estudio y sobre el que se quiere hacer la inferencia (los $\dots$ indican que en el gráfico no están representados todos los valores). Cada planta es una _unidad de muestreo_ porque la elegimos individualmente, y una _unidad observacional_ porque a cada planta le medimos la altura (observación). Aquellas alturas en rojo corresponden a las 6 plantas que seleccionamos mediante un _muestreo aleatorio_ para componer la muestra ($n = 6$). La altura promedio de las plantas de la muestra (_estadístico_) es $\bar{y} = 1.728$. Dado que el muestreo fue realizado al azar, el estadístico calculado a partir de la muestra brinda información sobre lo que pasa a nivel poblacional ($\mu$). A su vez, la aleatoriedad del muestreo determina que si repitieramos el muestreo de 6 unidades muchas veces, el estadístico muestral tomaría distintos valores cada vez ya que es una _variable aleatoria_. Finalmente, a partir de la _distribución de probabilidades_ asociada a los posibles valores que tomaría el estadístico muestral podemos realizar la inferencia sobre el parámetro poblacional incorporando la variabilidad del muestreo.


## Estadística descriptiva

La Estadística Descriptiva la rama de la Estadísica que comprende las técnicas y métodos para organizar, resumir y describir conjuntos de datos de distinto tipo recolectados en muestreos o censos. 

### Tablas de frecuencias

\newcommand{\LI}{\text{LI}}
\newcommand{\LS}{\text{LS}}
\newcommand{\MC}{\text{MC}}

Conocer la distribución de los valores de las variables en estudio es importante para examinar algunas características tales como la tendencia central, la dispersión, presencia de valores atípicos, etc. 

Una forma de examinar al distribución es mediante _tablas de frecuencias_ que consisten en el listado de los valores de la variable (individuales o agrupados en **clases**) y sus **frecuencias** o conteos correspondientes. Es útil para resumir variables categóricas, discretas o conjuntos de datos cuantitativos contínuos grandes $n > 25$. 

Las frecuencias representan el número de veces que un valor o clase está representado en la muestra o población. Para ejemplificar la construcción de tablas de frecuencias en **R** usaremos datos de pesaje de terneros de un establecimiento ganadero del norte de Santa Fe. Los datos encuentran en el archivo [`pesada_terneros.xlsx`](https://github.com/agustin-alesso/intro_DOE_R/raw/master/data/pesada_terneros.xlsx). 

```{r, eval = F, echo = T}
# Cargar los datos
library(readxl)
terneros <- read_excel("pesada_terneros.xlsx")
terneros
```

```{r, eval = T, echo = F}
# Cargar los datos
library(readxl)
terneros <- read_excel("./data/pesada_terneros.xlsx")
terneros
```

Primero hay que definir el número de ***intervalos de clases** y sus **límites de clase** (inferior $\LI$ y superior $\LS$). Generalmente se consideran semiabiertos a derecha `[ )` y si bien no hay un criterio estricto para determinar el número de clases, se recomiendan entre 5 y 20 clases de igual amplitud es adecuado. El número depende de la dispersión total de los datos y la cantidad de datos.

Una forma de hacerlo es mediante la función `pretty()` que utiliza un algoritmo para obtener números de clases que tengan límites _redondo_. Con el argumento `n = 8` se le sugiere un número tentativo de clases. El algoritmo en función de la variabiliad de los datos puede sugerir más o menos intervalos.

```{r, eval = T, echo = T}
# Definir los intervalos
lim <- with(terneros, pretty(Peso, n = 8))
lim
```

La diferencia o distancia entre dos $\LI$ o $\LS$ consecutivos se denomina **amplitud de clase**, $c = \LI_{i} - \LI_{i-1}$ y la **marca de clase** es el valor central de la clase, i.e. promedio entre los límites o bien $\MC = \LI + 0.5 c$. En este caso la amplitud de las clases sugeridas por `pretty()` es de 50 kg. Las marcas de clases se obtienen sumando media amplitud a los límites inferiores.

```{r}
c <- 50
mc <- lim[1:7] + 0.5 * c
mc
```

con los límites definidos se debe categorizar o discretizar la variable contínua, es decir, transformar el vector `Peso` en un vector contenga el intervalo al que pertenece cada valor de peso observado. Esto se hace con la función `cut()` indicando los límites de clase en `breaks`. El argumento `include.lowest = T` es para aseguramos que todos los datos sean incluidos en las clases y `right = F` para que los intervalos no sean cerrados a derecha, es decir, que sean semi-abiertos a derecha. Con la función `mutate()` de `dplyr` creamos una nueva columna en el set de datos con las clases que contiene.

```{r}
# Dividir los datos en clases
library(dplyr)
terneros <- mutate(terneros, clases = cut(Peso, breaks = lim, include.lowest = T, right = F))
terneros
```

De este modo cada valor de peso queda asociado a una clase. Luego haciendo el conteo de clases se obtiene las frecuencias usando `n`

```{r}
# Obtener frecuencias simples absolutas
tabla <- count(terneros, clases)
tabla
```

como no aparece la categoría `[300, 350)` hay que agregarla usando `complete()` del paquete `tidyr`.

```{r}
# Rellenar con clases faltantes
library(tidyr)
tabla <- complete(tabla, clases, fill = list(n = 0))
tabla
```

Esta tabla representa la distribución de los valores de cos de terneros. Se observa que la mayoría de los animales registró un peso de entre 150 y 200 kg ya que es el intervalo más frecuente. Existen algunos pocos terneros con cos llamativamente bajos y altos. La gran mayoría de los cos fueron entre 100 y 250 kg.

Las frecuencias definidas hasta aquí se denominan **frecuencias simples absolutas**  ya que representan el conteo de las cases. Cuando ese conteo se expresa en relación al total pasan a ser **frecuencias simples relativas** ($h_i = n_i/n$). Estas frecuencias pueden se pueden expresar de manera que muestren el número de veces que un valor o clase y los anteriores están representado en la muestra o población. Así se obtienen las **frecuencias acumuladas absolutas** ($F_i$) y **relativas** ($H_i = F_i/n$).

Para agregar estas frecuencias y marcas de clase: 

```{r}
# Agregar otras frecuencias
tabla <- mutate(tabla, mc = mc, F = cumsum(n), h = n/sum(n), H = F/sum(n))
tabla
```

Esta tabla nos permite conocer cerca del 50% de los datos estan contenidos en el rango `[150, 200)` kg.

Los comandos anteriormente vistos se pueden encadenar con el operador `%>%` para obtener de manera más compacta la tabla anterior:

```{r}
# Paquetes necesarios
library(dplyr)
library(tidyr)

# Limites y marcas de clase
lim <- pretty(terneros$Peso, n = 8)
c <- 50
mc <- lim[1:7] + 0.5 * c

# Tabla de frecuencias
tabla <- terneros %>%
  mutate(clases = cut(Peso, breaks = lim, include.lowest = T, right = F)) %>%
  count(clases) %>%
  complete(clases, fill = list(n = 0)) %>%
  mutate(mc = mc, F = cumsum(n), h = n/sum(n), H = F/sum(n))
tabla
```

### Gráficos


La distribución de los datos también se puede representar de manera gráfica. Existen varias técnicas que se complementan. 

#### Histograma

El **histograma** es la representación gráfica de la tabla de frecuencias vista anteriormente. Consiste en graficar las frecuencias (simples, absolutas o relativas) mediante barras sobre un eje horizontal que representa la variable en estudio. La altura de las barras representa las frecuencias y el ancho la amplitud del intervalo de clase.

En **R** mediante el paquete `ggplot2` podemos graficar histogramas con distinta cantidad y amplitud de clases combinando los argumentos `bins` y `binwidth`. Por ejemplo, un histograma con 8 clases para los datos peso de terneros sería:

```{r}
library(ggplot2)
p <- ggplot(terneros) + aes(x = Peso)
p + geom_histogram(bins = 8)
```

Achicando el ancho de clases:

```{r}
p + geom_histogram(binwidth = 25)
```

En el caso de variabls discretas, las barras se grafican separadas. Supongamos que queremos graficar las frecuencias de la variable `Procedencia`.

```{r, echo = F}
p <- ggplot(terneros) + aes(x = Procedencia)
p + geom_bar() + labs(y = "Frecuencia")
```

### Medidas de resumen

Las medidas de resumen son funciones que permiten extraer información relevante de la distribución de los datos y expresarla de manera resumida mediante números. De acuerdo a la característica de la distribución que resumen se pueden clasificar en medidas de: **tendencia central**, **dispersion**, **posición**  y **forma**

**Algo de notación**

Si $X$ es un vector que representa una muestra con $n$ observaciones, cada elemento de $X$ se identifica por su orden mediante $X_i$ donde $i$ es la posición de la i-ésima observación.

**Ejemplo:** si $X = [1.73, 1.84, 1.92, 2]$ son las alturas de 4 plantas de maiz, la altura de la planta 3 es $X_i = 1.92$

**R** representa los datos de manera parecida:

```{r}
x <- c(1.73, 1.84, 1.92, 2)
x[3]
```


#### Medidas de tendencia central

Resumen comportamiento de los datos más frecuentes, hacia donde se centra la distribución. Existen varias medidas las más comunes son la media, mediana y modo.

La **media aritmética**, es el centro de gravedad de los datos. Se define como el cociente entre la suma de los valores de la muestra y la cantidad observaciones:

$$
\bar{X} = \dfrac{1}{n} \sum_i^n x_i = \dfrac{\sum x_i}{n} = \dfrac{x_1 + x_2 + \cdots + x_n}{n}
$$

**Ejemplo:** Los cos de 5 novillos son: 250, 230, 280, 235, 260. El peso promedio es:

$$
\bar{X} = \dfrac{250 + 230  + 280 + 235 + 260}{5} = `r mean(c(250, 230, 280, 235, 260))`
$$

Es la medida más común para representar la tendencia central de los datos y es fácil de calcular. Es un buen estimador de la media poblacional ($\mu$). No obstante, es sensible a los valores extremos.

En **R** la función que calcula la media se denomina `mean()`

```{r}
# peso promedio
mean(terneros$Peso)
```

En promedio los terneros pesaron l82.65 kg.

Por su parte la **mediana** es el valor que separa al conjunto de datos (ordenados) en dos partes iguales. Es una medida robusta ya que no es afectada por los valores extremos.

$$ 
\tilde{X} = \begin{cases} \dfrac{x_{n/2} + x_{(n/2)+1}}{2} &\text{si n es par} \\
x_{(n+1)/2} &\text{si n es impar}
\end{cases}
$$

Si $n$ es par, $\tilde{X}$ es el promedio de los valores centrales (posiciones $n/2$ y $(n/2)+1$). Si $n$ es impar, $\tilde{X}$ es el valor ubicado en la posicion $(n+1)/2$

En **R** la función que calcula la mediana se denomina `median()`:

```{r}
# peso mediano
median(terneros$Peso)
```

El 50% de los terneros pesó igual o menos de 180 kg.

#### Medidas de dispersión 

Son medidas que resumen el grado de variabilidad de los datos. Entre las más comunes se encuentran el **rango**, **varianza** y **desviación estándar**.

El **rango** se define como la diferencia entre el valor máximo y mínimo de los datos. Es una medida muy simple pero altamente sensible a valores extremos ya que solamente usa la información del mínimo y máximo.

$$
\text{rango} = \text{max}(X) - \text{min}(X)
$$

En **R** el rango se obtiene de la misma manera mediante las funciones `max()` y `min()`.

```{r}
# Calculo del rango
with(terneros, max(Peso) - min(Peso))
```

Una medida mejor para expresar la variabilidad es la **varianza** ($s^2$) que se define como el promedio de las desviacione cuadráticas de las observaciones respecto de la media. 

$$
s^2 = \dfrac{\sum (x_i - \bar{X})^2}{n-1} = \dfrac{1}{n-1} \left[ \sum x_i^2 - \dfrac{(\sum x_i)^2}{n} \right]
$$

El denominador $n-1$ representa los grados de libertad, es decir, el número de datos de la muestra que pueden variar cuando se usa el estimador de la media $\bar{X}$. 

Dado que los desvíos respecto del promedio tienen la propiedad de cancelarse $\sum (x_i - \bar{x}) = 0$ los desvíos se deben elevar al cuadrado y esto a convierte una medida muy sensible a valores extremos.

La **desviación estándar** ($s$) es la raiz cuadrada de la varianza 

$$
s = \sqrt{s^2}
$$

La desviación estándar es interpretable ya que está en la escala original. Al igual que la varianza es sensible a valores extremos ya que utiliza $\bar{X}$ y los desvíos elevados al cuadrado.

En **R** la varianza y desvio se obtienen con `var()` y `sd()`.

```{r}
# Varianza
var(terneros$Peso)

# Desvío
sd(terneros$Peso)
```

Si bien la desviación estándar es interpretable ya que está en la misma escala que la variable original, a veces es conveniente expresarle en relación al promedio de los datos para compararlo con otros datos o variables. El **coeficiente de variación** ($CV$) es una media de dispersión relativa para comparar la variabilidad entre muestras.

$$
CV = \dfrac{s}{\bar{X}}
$$

En **R** podemos obtenerlo combinando las funciones `sd()` y `mean()`

```{r}
# CV de los cos
with(terneros, sd(Peso)/mean(Peso))
```

Los pesos de los terneros varían un 20% en torno al promedio.

#### Medidas de posición

Los cuantiles son una generalización del concepto de la mediana. En **R** se calculan con la función `quantile()`. Cuando los datos se dividen en cuatro partes iguales se llaman **cuartiles** $(Q_i)$, si se divide en 10 partes iguales son **deciles** ($D_i$)  y para 100 partes iguales **percentiles** ($P_i$). 

$$
\tilde{X} = Q_2 = D_5 = P_{50}
$$

Una medida importante es el rango intercuartílico ($IQR$) que indica entre que valores se encuentra el 50% central de la distribución de los datos

$$
IQR = Q_3 - Q_1
$$

Por ejemplo para obtener los valores de peso que separa a la muestra en 4 partes iguales:

```{r}
with(terneros, quantile(Peso, probs = c(0.25, 0.50, 0.75)))
```

o bien, ¿cuánto pesa el ternero más liviano del 20% de los terneros más pesados? Eso sería el percentil 0.8.

```{r}
with(terneros, quantile(Peso, prob = 0.8))
```

Para obtener el percentil que corresponde a un dato particular de la muestra se usa la _función empírica de distribución acumulada_.

```{r}
# Obtener la ECDF
ecdf_peso <- ecdf(terneros$Peso)

# Obtener el percentil del peso = 213.2
ecdf_peso(213.2)
```

### Gráfico de caja

Es una técnica propuesta por John Tukey para representar la distribución de los datos mediante un gráfico  exploratorio basado en medidas robustas (mediana, cuartiles, etc). Permite ver la región central de los datos, el sesgo y la presencia datos atípicos leves y moderados

```{r, echo = F, fig.height=2}
set.seed(1)
x <- c(rnorm(30, mean = 20, sd = 5), 40, 50)

# Medidas
q <- boxplot.stats(x)$stats
outs <- boxplot.stats(x)$out
riq <- IQR(x)

# Vallas
VEI <- q[2] - 3*riq 
VES <- q[4] + 3*riq 
VII <- q[2] - 1.5*riq 
VIS <- q[4] + 1.5*riq 

# Gráfico
boxplot(x, horizontal = T, range = 1.5, ylim = c(0, 55))
abline(v = c(VEI, VII, VIS, VES), lty = 2)
text(c(VEI, VII, VIS, VES), 1.5, labels = c("VEI", "VII", "VIS", "VES"),pos = c(2,2, 4,4))
text(q[3],1.25, labels = "Mna")
text(c(q[2], q[4]), 0.75, labels = c(expression(Q[1]), expression(Q[3])))
segments(x0 = q[2], y0 = 0.7, x1 = q[4], y1 = 0.7)
text(q[3], 0.7, labels = "RIQ", pos = 1)
text(q[c(1, 5)],1.1, labels = "Ady", pos =3)
text(outs,1, labels = c("outlier \n leve", "outlier \n leve", "outlier \n extremo"), pos = 1)
```


En el caso de los terneros, la distribución de los pesos se puede representar con un gráfico de cajas usando `geom_boxplot()`

```{r, fig.height = 2}
# Boxplot peso terneros
ggplot(terneros, aes(x = "", y = Peso)) +
  geom_boxplot() + 
  coord_flip()
```

Según este gráfico el 50% de la distribución esta entre `r round(quantile(terneros$Peso, 0.25), 2)` y `r round(quantile(terneros$Peso, 0.75), 2)` y el sesgo es pequeño. Por defecto el gráfico de caja muestra los valores atípicos leves y extremos ya que toma 1.5 veces el rango intercuartilico. Para determinar si hay valores atípicos extremos se puede usar el argumento `coef = 3`.

```{r, fig.height = 2}
# Boxplot peso terneros con rango = 3
ggplot(terneros, aes(x = "", y = Peso)) +
  geom_boxplot(coef = 3) + 
  coord_flip()
```

El valor `peso = 384` es un outlier extremo.

### Medidas de forma 

Las medidas de forma resumen otros aspectos de la distribución como el sesgo o asimetría o la variabilidad en distribuciones unimodales o kurtosis. 

La **asimetría** de una distribución hace referencia a como se distribuyen los valores en torno a la media.
En una distribución simétrica, la mitad izquierda es identica a la mitad derecha.

```{r, echo = F, fig.height = 3}
set.seed(1)
x <- seq(0, 1, by = 0.01) 
x_sim <- dbeta(x, 5, 5)
x_neg <- dbeta(x, 5, 2)
x_pos <- dbeta(x, 2, 5)
par(mfcol = c(1,3), xaxt = "n", yaxt = "n")
plot(x, x_sim, main = "Media = Mediana = Modo", xlab = "", ylab = "", type = "l")
abline(v = c(0.5, 0.5), lty = c(1,2))
plot(x, x_neg, main = "Media > Mediana > Modo", xlab = "", ylab = "", type = "l")
abline(v = c(0.8, 0.6), lty = c(1,2))
plot(x, x_pos, main = "Media > Mediana > Modo", xlab = "", ylab = "", type = "l")
abline(v = c(0.4, 0.2), lty = c(1,2))
```

La **kurtosis** indica cuanto se concentran los valores en torno a la media. Las distribuciones leptocurticas tienen menor variabilidad. Lo opuesto ocurre con las platicurticas.

```{r, echo = F, fig.height = 3}
set.seed(1)
x_normal <- rnorm(100, 50, 10)
x_plat <- rnorm(100, 50, 40)
x_lept <- rnorm(100, 50, 1)
par(mfcol = c(1,3), xaxt = "n", yaxt = "n")
plot(density(x_normal, bw = 30), main = "Normal", xlab = "", ylab = "", xlim = c(-50, 150)) 
plot(density(x_plat, bw = 30), main = "Platicurtica", xlab = "", ylab = "", xlim = c(-50, 150))
plot(density(x_lept, bw = 30), main = "Leptocurtica", xlab = "", ylab = "", xlim = c(-50, 150))
```

Hay varias formas de calcular los coeficientes de asimetría y kurtosis. Una de ellas deriva de los momentos estandarizados:

$$
m_k = \dfrac{\sum (X_i - \bar{X})^k}{n}
$$

Si $k = 1$, $m = 0$ dado que $\sum (X_i - \bar{X}) = 0$. En cambio, si $k = 2$, $m$ es igual al estimador sesgado de la varianza poblacional. Así los momentos 3 y 4 tienen información sobre la asimetría y curtosis.

El coeficiente $\sqrt{b_1}$ indica la asimetría:

$$
\sqrt{b_1} = \dfrac{m_3}{\sqrt{m_2^3}} = \dfrac{\sqrt{n} \sum (X_i - \bar{X})^3}{\sqrt{ \left[ \sum(X_i - \bar{X})^2 \right]^3}}
$$

- Si $\sqrt{b_1} = 0$ la distribución es simétrica
- Si $\sqrt{b_1} > 0$ la distribución es sesgada a derecha
- Si $\sqrt{b_1} < 0$ la distribución es sesgada a izquierda

El coeficiente $b_2$ indica la kurtosis

$$
b_2 = \dfrac{m_4}{m_2^2} = \dfrac{n \sum (X_i - \bar{X})^4}{\left[ \sum(X_i - \bar{X})^2 \right]^2}
$$

- Si $b_2 = 3$ la distribución es normal
- Si $b_2 > 3$ la distribución es leptocirtoca
- Si $b_2 < 3$ la distribución es platocurtica

En **R** los coeficientes de kurtosis y asimetría se obtienen del as funciones `skewness()` y `kurtosis()` del paquete `moments`:

```{r}
library(moments)

# asimetría
skewness(terneros$Peso)

# kurtosis
kurtosis(terneros$Peso)
```


### Todas las medidas juntas

Usando la función `summarise()` de `dplyr` se pueden calcular una o más estadísticas de resumen y resumirlas en forma de tabla. Asímismo también pueden calcularse por subconjuntos.

```{r}
library(moments)

# General
terneros %>%
  summarise(n = length(Peso), media = mean(Peso), mediana = median(Peso), desv = sd(Peso),
            min = min(Peso), max = max(Peso), IQR = IQR(Peso), Q20 = quantile(Peso, 0.2),
            Q80 = quantile(Peso, 0.8), asim = skewness(Peso), kurt = kurtosis(Peso))

# Estadítica de resumen por Procedencia
terneros %>%
  group_by(Procedencia) %>% 
  summarise(n = length(Peso), media = mean(Peso), mediana = median(Peso), desv = sd(Peso),
            min = min(Peso), max = max(Peso), IQR = IQR(Peso), Q20 = quantile(Peso, 0.2),
            Q80 = quantile(Peso, 0.8), asim = skewness(Peso), kurt = kurtosis(Peso))
```

## distribuciones (directamente la normal y TCL)

## IC

## Test hipotesis: generalidades

Las pruebas de hipótesis es uno de los métodos de inferencia estadística. A diferencia de la estimación de parámetros, en una prueba de hipótesis hay una idea previa del valor del un parámetro poblacional que será contrastado con los datos obtenidos de esa población.

En una prueba de hipótesis se plantean **dos modelos o hipótesis** para explicar el fenómeno o proceso en estudio que produce los datos (_población_): 

$$
H_0: \theta = \theta_0 \quad \text{vs} \quad H_1: \theta \ne \theta_0
$$

La **hipótesis nula** $H_0$ es la que se usa para construir el modelo de la población con la idea de verificar su falsedad. Refleja un pensamiento escéptico. La **hipótesis alternativa o de investigación** $H_1$ es la alternativa a considerar en caso de rechazar $H_0$. La _evidencia empírica_ que sustenta o rechaza el modelo nulo $H_0$ se obtiene de una muestra aleatoria de la población sobre la cual se quiere inferir. 

Una prueba de hipótesis estadística se basa en el concepto de **prueba por contradicción**, es decir que se evalúa la $H_1$ probando o no la falsedad de $H_0$. Esto involucra:

1. Formular las hipótesis $H_1$ y $H_0$
2. Cálcular el estadístico de prueba apropiado, e.g. $Z_{\theta_0}$
3. Determinar la zona de rechazo y regla de decisión.
4. Chequear de supuestos
5. Concluir 

### ¿Cómo se formulan las hipótesis? 

Las hipótesis pueden formularse teniendo los siguientes criterios generales:

1. La afirmación de que $\theta$ es igual a un determinado valor **siempre** tiene que estar incluida en $H_0$. El valor hipotético de $\theta$ en $H_0$ es el **valor nulo** $\theta_0$
2. La afirmación sobre $\theta$ que se quiere soportar or detectar con los datos es la $H_1$
3. La negación de $H_1$ es la $H_0$
4. La $H_0$ se presume correcta a menos que hay evidencia suficiente para rechazarla en pos de la $H_1$

**Ejemplos 1** Un edafólogo quiere evaluar si el contenido de N total de los suelos de una región es igual o inferior a 0.12%. Como su hipótesis de trabajo contiene la igualdad, entonces $H_0: \mu \le 0.12\% \quad \text{vs} \quad H_1: \mu > 0.12\%$

**Ejemplos 2** Otro edafólogo quiere evaluar si el contenido de N total es menor a 0.12%. En este caso, la hipótesis de trabajo no incluye la igualdad, entonces: $H_0: \mu \ge 0.12\% \quad \text{vs} \quad H_1: \mu < 0.12\%$.

**Ejemplo 3** A un tercero le interesa saber si el nivel de N total es distinto de 0.12%. Como la hipotesis de trabajo plantea la desigualdad, entonces, $H_0: \mu  = 0.12\% \quad \text{vs} \quad H_1: \mu \ne 0.12\%$

### Estadístico de prueba

El estadísico de prueba es un valor estandarizado que se calcula a partir de los datos de la muestra aplicando una función. Como la muestra es un conjunto de variables aleatorias, el resultado de la función aplicada es a su vez una **variable aleatoria** cuyos posibles valores tienen una distribución de probabilidades asociada la cual está determinada por la $H_0$ y los supuestos

```{r, echo = F, fig.align = 'center'}
z <- seq(-4, 4, by =0.01)
d <- dnorm(z)
alpha <- 0.05
z_crit <- qnorm(p = 0.8)
lbl_alfa <- c(expression(1-p), expression(p))

# Gráfico
plot(z, d, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA)

# Area
z_coords <- c(z_crit, z[z <= z_crit])
y_coords <- c(0, d[z <= z_crit])
polygon(x = z_coords, y = y_coords, col = "gray")
segments(x0 = z_crit, y0 = 0, x1 = z_crit, y1 = dnorm(z_crit))

# Anotaciones
lbl1 <- c(NA, expression(paste(H[0], ": ", theta, " = ", theta[0])), expression(Z[theta[0]]), NA)
axis(side = 1, at = c(-6, 0, z_crit, 6), labels = lbl1)
abline(v = 0, lty = 2)
```

### Zona de rechazo

La zona de rechazo está compueta por los valores de la distribución del estadístico de prueba $Z_\theta$ que soportan la $H_1$ y contradicen $H_0$, es decir, los valores de $\theta$ con que permiten **rechazar** $H_0$.

```{r, echo = F, fig.align = 'center'}
z <- seq(-4, 4, by =0.01)
d <- dnorm(z)
alpha <- 0.05
z_crit <- qnorm(p = 1-alpha/2)
lbl_alfa <- c(expression(1-p), expression(p))

# Gráfico
plot(z, d, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA)
segments(x0 = z_crit, y0 = -0.5, x1 = z_crit, y1 = dnorm(z_crit), xpd = T)

# Area
z_coords <- c(z_crit, z[z >= z_crit])
y_coords <- c(0, d[z >= z_crit])
polygon(x = z_coords, y = y_coords, col = "gray")

# Anotaciones
lbl1 <- c(NA, expression(paste(H[0], ": ", theta, " = ", theta[0])), expression(Z[theta[0]]), NA)
axis(side = 1, at = c(-6, 0, 6), labels = lbl1[-3])
axis(side = 1, at = c(z_crit), labels = lbl1[3], hadj = 1.2)
abline(v = 0, lty = 2)
mtext(text = c("Zona de Aceptación", "Zona de Rechazo"), side = 1, line = 2,
      at = c(z_crit - 1.5, z_crit +1.5))
```

La $H_1$ determina donde está la zona de rechazo y el sentido de la interpretación de la evidencia. De este modo las pruebas pueden ser bilaterales si $H_1: \theta \ne \theta_0$ o unilaterales izquierda $H_1: \theta < \theta_0$ o derecha $H_1: \theta > \theta_0$

```{r, echo = F, fig.align = 'center', fig.height = 3, fig.width = 10}
z <- seq(-4, 4, by =0.01)
d <- dnorm(z)
alpha <- 0.05
z_crit <- qnorm(p = c(alpha/2, 1-alpha/2))

par(mfcol = c(1,3), cex.axis = 1.8)

# Gráfico unilateral izquierda
plot(z, d, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA,
     main = "unilateral a izquierda")
segments(x0 = z_crit[1], y0 = 0, x1 = z_crit[1], y1 = dnorm(z_crit[1]), xpd = T)
z_coords <- c(z_crit[1], z[z <= z_crit[1]])
y_coords <- c(0, d[z <= z_crit[1]])
polygon(x = z_coords, y = y_coords, col = "gray")
lbl1 <- c(NA, expression(Z[alpha]), expression(1-Z[alpha/2]), NA)
axis(side = 1, at = c(-6, z_crit[1], 6), labels = lbl1[-3])

# Gráfico bilateral
plot(z, d, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA,
     main = "bilateral")
segments(x0 = z_crit, y0 = 0, x1 = z_crit, y1 = dnorm(z_crit), xpd = T)
z_coords1 <- c(z_crit[1], z[z <= z_crit[1]])
y_coords1 <- c(0, d[z <= z_crit[1]])
z_coords2 <- c(z_crit[2], z[z >= z_crit[2]])
y_coords2 <- c(0, d[z >= z_crit[2]])
polygon(x = z_coords1, y = y_coords1, col = "gray")
polygon(x = z_coords2, y = y_coords2, col = "gray")
lbl1 <- c(NA, expression(Z[alpha/2]), expression(Z[1-alpha/2]), NA)
axis(side = 1, at = c(-6, z_crit, 6), labels = lbl1)

# Gráfico unilateral derecha
plot(z, d, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA,
     main = "unilateral a derecha")
segments(x0 = z_crit[2], y0 = 0, x1 = z_crit[2], y1 = dnorm(z_crit[2]), xpd = T)
z_coords <- c(z_crit[2], z[z >= z_crit[2]])
y_coords <- c(0, d[z >= z_crit[2]])
polygon(x = z_coords, y = y_coords, col = "gray")
lbl1 <- c(NA, expression(Z[1-alpha/2]), NA)
axis(side = 1, at = c(-6, z_crit[2], 6), labels = lbl1)
```

### Errores

En toda prueba de hipótesis se pueden cometer errores al tomar la decisión. El tipo de error dependerá de la decisión tomada y el valor verdadero de la $H_0$. 

            Decisión  $H_0$ es cierta   $H_0$ es falsa
-------------------- ----------------- ----------------
    se rechaza $H_0$  Error Tipo I      No hay error  
 no se rechaza $H_0$  No hay error      Error Tipo II 
-------------------- ----------------- ----------------

Si bien no podemos conocer el verdadero valor del los parámetros poblacionales, y por lo tanto desconocemos si la $H_0$  es verdadera o falsa, es posible estimar las probabilidades de cometer los errores Tipo I y tipo II, las que se denominan $\alpha$ y $\beta$ respectivamente.

Supongamos que tenemos dos hipótesis en juego $H_0: \mu = 1$ vs $H_1: \mu = 1$. El siguiente gráfico muestra las áreas sombreadas $\alpha$ y $\beta$ que representan la probabilidad de cometer error tipo I y II si tomamos la información del estadísico $z_0$ para decidir sobre $H_0$. Si el modelo válido fuera el especificado en $H_0$, el área $\alpha$ indica la probabilidad de rechazar $H_0$ cuando es verdadera. En cambio, si $H_1$ fuese verdadera, el área $\beta$ representa la probabilidad de no rechazar $H_0$ cuando esta es falsa.

```{r, echo = F, fig.align = 'center', fig.height = 2, fig.cap="Distribución de probabilidades de estadístico de prueba bajo supuesto de H0 y H1"}
par(mar = c(2, 0, 0, 0))
z <- seq(-4, 4, by =0.01)
d1 <- dnorm(z, mean = -1)
d2 <- dnorm(z, mean = 1)
alpha <- 0.05
z_calc <- 0.45
lbl_alfa <- c(expression(beta), expression(alpha))

# Gráfico
plot(z, d1, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA)
lines(z, d2, lty = 2)
segments(x0 = z_calc, y0 = 0, x1 = z_calc, y1 = dnorm(z_calc, mean = 1))

# Area
z1_coords <- c(z_calc, z[z >= z_calc])
z2_coords <- c(z[z <= z_calc], z_calc)
y1_coords <- c(0, d1[z >= z_calc])
y2_coords <- c(d2[z <= z_calc], 0)
polygon(x = z1_coords, y = y1_coords, col = "gray")
polygon(x = z2_coords, y = y2_coords, col = "light gray")

# Anotaciones
lbl1 <- c(NA, expression(z[0]), NA)
axis(side = 1, at = c(-6, z_calc, 6), labels = lbl1)
#abline(v = c(-1, 1), lty = 2)
text(x = c(-0.5, 1.5), y = c(0.05, 0.05), labels = lbl_alfa)
text(x = c(-2, 2), y = c(0.35, 0.35), labels = expression(paste("H"[0], ": ", mu, " = 0"), paste("H"[1], ": ", mu, " = 1")))
```


$$
P(\text{rechazar $H_0$ | $H_0$ es cierta}) = \alpha \\
P(\text{no rechazar $H_0$ | $H_0$ es falsa}) = \beta
$$

$\alpha$ y $\beta$ dependen de la **verdadera distancia entre las medias**, la **variabilidad de la población** en estudio y el **tamaño de la muestra**.

Así como $\beta$ determina las probabilidades de cometer error tipo II, el complemento $1-\beta$ representa la probabilidad que tenemos de decidir correctamente sobre $H_0$ rechazándola cuando realmente es falsa. Esto se denomina **potencia**

```{r, echo = F, fig.align = 'center', fig.height = 3}
par(mar = c(2, 0, 0, 0))
z <- seq(-4, 4, by =0.01)
d1 <- dnorm(z, mean = -1)
d2 <- dnorm(z, mean = 1)
alpha <- 0.05
z_calc <- 0.45
lbl_alfa <- c(expression(1-beta), expression(alpha))

# Gráfico
plot(z, d1, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA)
lines(z, d2, lty = 2)
segments(x0 = z_calc, y0 = 0, x1 = z_calc, y1 = dnorm(z_calc, mean = 1))

# Area
z1_coords <- c(z_calc, z[z >= z_calc])
z2_coords <- c(z[z >= z_calc], z_calc)
y1_coords <- c(0, d1[z >= z_calc])
y2_coords <- c(d2[z >= z_calc], 0)
polygon(x = z1_coords, y = y1_coords, col = "gray")
polygon(x = z2_coords, y = y2_coords, density  = 30)

# Anotaciones
lbl1 <- c(NA, expression(z[0]), NA)
axis(side = 1, at = c(-6, z_calc, 6), labels = lbl1)
#abline(v = c(-1, 1), lty = 2)
text(x = c(1.5, .75), y = c(0.15, 0.025), labels = lbl_alfa)
text(x = c(-2, 2), y = c(0.35, 0.35), labels = expression(paste("H"[0], ": ", mu, " = 0"), paste("H"[1], ": ", mu, " = 1")))
```

$$
P(\text{rechazar $H_0$ | $H_0$ es falsa}) = 1 - \beta
$$

Para un nivel de significancia $\alpha$ dado, la potencia se halla utilizando la distribución que asume $H_1$ como verdadera.

### Regla de decisón y valores $p$

**Opción 1**

Fijar una valor de $\alpha$ (probabilidad de error Tipo I) y, calculando valores críticos del estadísitco $Z_\theta$, delimitar las zonas de rechazo o aceptación. 

Para bilaterales:

- Si $|Z_\theta| \ge z_{1-\alpha/2}$ se rechza $H_0$
- Si $|Z_\theta| < z_{1-\alpha/2}$ no se rechza $H_0$

Para unilaterales izquierda

- Si $Z_\theta \ge z_{\alpha}$ no se rechza $H_0$ 
- Si $Z_\theta < z_{\alpha}$ se rechza $H_0$

Para unilaterales derecha

- Si $Z_\theta \le z_{1-\alpha}$ no se rechza $H_0$
- Si $Z_\theta > z_{1-\alpha}$ se rechza $H_0$

**Opción 2**

Asumiendo $H_0$ y una distribución del estadístico (e.g. Normal), se calcula la probabilidad $p$ de ocurrencia de un valor igual o más extremo (a una o dos colas).

$$
P(Z \ge Z_{\theta_0} | H_0) = p
$$

Luego:

- Si $p > \alpha$, **no hay evidencias para rechazar $H_0$**, aunque no se prueba que ésta sea verdadera

- Si $p \le \alpha$, **hay evidencia para rechazar $H_0$**, aunque no se esta probando que $H_0$ sea falsa o $H_1$ sea verdadera

**¿Qué es el valor p?**

Es la probabilidad de que, si se repite el muestreo, el estadístico de prueba a obtener sea igual o más extremo que el observado en los datos asumiendo que $H_0$ es cierta.

_Si los datos no concuerdan con el modelo nulo, entonces hay evidencia para descartarlo en favor del alternativo_

### Conclusiones

La conclusión consta de 3 partes fundamentales:

1. ¿Cuál es la decisión? ¿Que sucede con $H_0$?

2. ¿Por qué se llega a esa decisión? El estadístico con valor p o bien con el valor crítico y nivel de significancia.

3. Interpretación o consencuencias en el contexto del problema o pregunta de investigación

Una inferencia completa es cuando además de contestar sobre 1 y 2 se agrega información sobre las consecuencias de haber recvhazado $H_0$ en el contexto del problema de investigación. Agregar información sobre la magnitud y sentido de las diferencias agregar más información a la inferencia.

## Comparación de dos poblaciones

### Muestras independientes

```{r, echo = F, fig.height = 4}
par(mar = c(2, 0, 0, 0))
n <- 10
ybar1 <- 32
ybar2 <- 38
ybar <- mean(c(ybar1, ybar2))
se1 <- 8/sqrt(n)
se2 <- 8/sqrt(n)
se <- se1+se2
x <- seq(20, 50, by = 0.01)
d1 <- dnorm(x, ybar1, se1)
d2 <- dnorm(x, ybar2, se2)
d3 <- dnorm(x, ybar, se)
```

¿Las muestras con $\bar{y}_1$ y $\bar{y}_2$ provienen de dos poblaciones distintas con $\mu_1$ y $\mu_2$, o de una sola población con $\mu$?

```{r, echo = F, fig.width=4, fig.height = 3.5}
par(mar = c(2, 0, 0, 0))
plot(x, d1, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA, ylim = c(0, 0.25))
lines(x, d2)
lbl <- c(expression(mu[1]), expression(mu[2]))
axis(side = 1, at = c(20, ybar1, ybar2, 50), labels = c(NA, lbl, NA))
segments(x0 = c(ybar1,ybar2), y0 = 0, x1 = c(ybar1, ybar2), y1 = dnorm(x, c(ybar1, ybar2), c(se1, se2)),
         lty = 2)
```

$$ E(\bar{y}_1) = \mu_1 \\
E(\bar{y}_2) = \mu_2 $$

```{r, echo = F, fig.width=4, fig.height = 3.5}
par(mar = c(2, 0, 0, 0))
plot(x, d3, type = "l", bty = "n", axes = F, yaxs = "i", ylab = '', xaxt = 'n', xlab = NA, ylim = c(0, 0.25))
axis(side = 1, at = c(20, ybar, 50), labels = c(NA, expression(mu), NA))
segments(x0 = ybar, y0 = 0, x1 = ybar, y1 = dnorm(x, ybar, se), lty = 2)
```
$$ E(\bar{y}_1) = \mu \\
E(\bar{y}_2) = \mu $$

### Modelo lineal aditivo 

Las observaciones se pueden representar con un modelo estadístico.

#### Modelo de medias de celda

$$
y_{ij} = \mu_i + e_{ij} \quad \quad i = 1, 2; j = 1, \dots, n_i
$$

donde: 

- $y_{ij}$ es la j-ésima observación del i-ésimo tratamiento

- $\mu_i$ es la media del i-ésimo tratamiento

- $e_{ij}$ es el error aleatorio asociado a la j-ésima observación del i-ésimo tratamiento

**Supuestos**: $e_{ij} \sim N(0, \sigma)$

1. Independencia: las observaciones son independientes, i.e. $Cor(e_{ij}, e_{i'j'}) = 0$
2. Normalidad: los residuos siguen una distribución normal
3. Homocedasticidad: las varianzas de las poblaciones son iguales

### Modelo de efectos

$$
y_{ij} = \mu + \tau_i + e_{ij} \quad \quad i = 1, 2; j = 1, \dots, n_i
$$

donde: 

- $y_{ij}$ es la j-ésima observación del i-ésimo tratamiento

- $\mu$ es la media general

- $\tau_i$ es la desviación o efecto del i-ésimo tratamiento respecto de $\mu$, es decir $\tau_i = \mu_i - \mu$

- $e_{ij}$ es el error aleatorio asociado a la j-ésima observación del i-ésimo tratamiento

**Supuestos:** $e_{ij} \sim N(0, \sigma)$

## Prueba hipótesis

$$
H_0: \mu_1 - \mu_2 = 0 \quad \text{o bien} \quad H_0: \tau_i = 0\\
H_1: \mu_1 - \mu_2 \ne 0 \quad \text{o bien} \quad  H_1:\tau_i \ne 0 \\
$$

**Estadístico de prueba**

$$
t = \dfrac{(\bar{y}_1 - \bar{y}_2) - (\mu_1 - \mu_2)}{S_{\bar{y}_1 - \bar{y}_2}} \sim t_{\nu}
$$

Donde $S_{\bar{y}_1 - \bar{y}_2}$ es un estimador muestral del $\sigma_{\mu_1 - \mu_2}$ y su forma de cálculo depende de si las dos poblaciones tienen varianza común $\sigma^2_1 = \sigma^2_2 = \sigma^2$

**Regla decisión**

- Si $| t | > t_{\nu, 1-\alpha/2}$, entonces se rechaza $H_0$
- Si $| t | \le t_{\nu, 1-\alpha/2}$, entonces se no rechaza $H_0$

## Si $\sigma^2_1 = \sigma^2_2$

Entonces $\sigma^2_{\bar{y}_1 - \bar{y}_2}$ se estima usando la información de $s^2_1$ y $s^2_2$ en una varianza _amalgamada_

$$
s^2_a = \dfrac{(n_1-1) s^2_1 + (n_2-1) s^2_2}{n_1 + n_2 - 2}
$$

y el estadístico es $t$ sigue una distribución $t$ con $n_1 + n_2 - 2$ grados de libertad

$$
t = \dfrac{(\bar{y}_1 - \bar{y}_2) - (\mu_1 - \mu_2)}{S_a \sqrt{\dfrac{1}{n_1} + \dfrac{1}{n_2}}} \sim t_{n_1 + n_2 - 2}
$$

## Si $\sigma^2_1 \ne \sigma^2_2$

$\sigma^2_{\bar{y}_1 - \bar{y}_2}$ se estima usando las varianzas muestrales sin combinar y el estadístico $t$ que tiene distribución $t$ con $\delta$ grados de libertad

$$
t = \dfrac{(\bar{y}_1 - \bar{y}_2) - (\mu_1 - \mu_2)}{\sqrt{\dfrac{s^2_1}{n_1} + \dfrac{s^2_2}{n_2}}} \sim t_\delta
$$

Los grados de libertad se aproximan por _Welch-Satterwaite_:

$$
\delta = \dfrac{\left(\dfrac{s^2_1}{n_1-1} + \dfrac{s^2_2}{n_2} \right)^2}
{\dfrac{1}{n_1-1} \left(\dfrac{s^2_1}{n_1} \right)^2 +
\dfrac{1}{n_2-1} \left(\dfrac{s^2_2}{n_2} \right)^2}
$$


## Ejemplo

Un mejorador de trigo desea comparar dos materiales experimentales (`Exp1` y `Exp2`) para determinar si sus rendimientos difieren significativamente fijando una probabilidad de cometer error tipo I de 5% ($\alpha$ = 0.05). Para ello, en un sector del campo experimental, delimita 20 parcelas de 2 x 4 m y siembra al azar en cada parcela una de las dos variedades. Luego, a cosecha, determina el rendimiento de cada parcela cortando espigas de 3 m^2^. Los rendimientos en qq ha^-1^ fueron:

```{r, echo = F}
library(MASS)
library(writexl)

# Simular datos
mu <- c(40, 35)
s <- c(10, 3)
r <- 0.9
Sigma <- matrix(prod(r, s), ncol = 2, nrow = 2)
diag(Sigma) <- s^2

set.seed(1)
trigo <- data.frame(
    Parcela = 1:10,
    Variedad = rep(c('Exp1', 'Exp2'), each = 10),
    rend = as.vector(round(mvrnorm(n = 10, mu = mu, Sigma= Sigma), 2))
)

write_xlsx(trigo, "./data/trigo.xlsx")
```

*Exp 1*

```{r, echo = F}
with(trigo, cat(rend[Variedad == 'Exp1'], sep = ", "))
```

*Exp 2*

```{r, echo = F}
with(trigo, cat(rend[Variedad == 'Exp2'], sep = ", "))
```

¿Cual es la variable respuesta? ¿Cuales son los tratamientos? ¿Cuantas repeticiones? ¿Existen diferencias significativas entre ambas variedades?

¿Cómo lo analizamos usando **R**?

##

Paso 0: descargar el archivo [`trigo.xlsx`](https://entornovirtual.unl.edu.ar/pluginfile.php/429184/mod_folder/content/0/trigo.xlsx?forcedownload=1) a la carpeta `data` del proyecto. 

Paso 1: importar los datos en **R** con el paquete `readxl`

```{r, echo = T, eval = T}
library(readxl)
trigo <- read_excel("./data/trigo.xlsx")
```

Paso 2: explorar los datos

```{r}
# Cargar dplyr
library(dplyr)

# Medias de resumen
res <- trigo %>%
  group_by(Variedad) %>%
  summarise(ybar = mean(rend), s = sd(rend), n = n())
res
```

## 

```{r, fig.width = 3, fig.height = 3.5}
# cargar paquetes ggplot2 y ggthemes y definir tema
library(ggplot2)
library(ggthemes)
theme_set(theme_bw())

# Gráfico de caja
ggplot(trigo, aes(x = Variedad, y = rend)) + geom_boxplot()  
```

¿Existen diferencias significativas entre ambas variedades?

## 

Paso 3: prueba de hipótesis

$$ H_0: \mu_1 - \mu_2 = 0 \quad \text{vs} \quad H_1: \mu_1 - \mu_2 \ne 0 $$

Si asumimos muestras independientes y varianzas homogéneas, bajo $H_0$ la varianza de los rendimientos sería:

```{r, echo = F}
ybar <- res$ybar
s <- res$s
n <- res$n
```

$$
\begin{align}
s^2_a &= \dfrac{(n_1-1) s^2_1 + (n_2 - 1) s_2^2}{n_1+n_2-2} = \\
&= \dfrac{(`r n[1]-1`) `r round(s[1]^2, 2)` + (`r n[2]-1`) `r round(s[2]^2, 2)`}{`r sum(n)-2`} = `r round(mean(s^2), 2)`
\end{align}
$$

Luego el estadístico de prueba es:

$$
\begin{align}
t &= \dfrac{(\bar{y}_1 - \bar{y}_2) - (\mu_1 - \mu_2)}{s_a \sqrt{\dfrac{1}{n_1} + \dfrac{1}{n_2}}} = \\
&= \dfrac{`r round(ybar[1],2)` - `r round(ybar[2],2)`}{`r round(sqrt(mean(s^2)), 2)`} = `r round((ybar[1]-ybar[2])/sqrt(mean(s^2*2/n[2])), 2)`
\end{align}
$$

Como $|t = 1.71| \le t_{1-0.05/2, 18} = `r qt(0.975, 18)`$, entonces no se rechaza $H_0$ 

##

En **R** usando la función `t.test()`...

```{r}
# Prueba t
t.test(rend ~ Variedad, trigo, paired = F, var.equal = T, alternative = 'two.sided')
```

*Conclusión*: la muestra no proporciona evidencia suficiente para rechazar $H_0$ al 5% de significancia, por lo tanto las diferencias de rendimiento entre `Exp1` y `Exp2` no son estadísticamente significativas (p = 0.1049). Con un 95% de confianza, se estima que la verdadera diferencia de rendimiento entre `Exp1` y `Exp2` está contenida en el intervalo -1.02 y 9.88 qq ha^-1^

## Verificación de supuestos

Este análisis supone $e_{iid} \sim N(0, \sigma)$ 

### Independencia

Chequear correlación de los $e_{ij}$, frecuentemente en espacio o tiempo. Una correcta aleatorización correcta disminuye las chances de obtener observaciones autocorrelacionadas

### Normalidad

Comparar la distribución de los $e_{ij}$ con una distribución Normal. Hay varias pruebas formales. _QQ plot_ es un gráfico de diagnóstico. 

### Homogeneidad de varianzas

Evaluar si las varianzas de ambos tratamientos son iguales o distintas. Es el supuesto más fuerte. Hay test formales y gráfico de diagnóstico

## 

Los supuestos se evalúan sobre los residuos $e_{ij} = y_{ij} - \mu_i$

```{r}
# calcular residuos
trigo <- trigo %>% 
  group_by(Variedad) %>% 
  mutate(ybar = mean(rend)) %>% 
  ungroup() %>% 
  mutate(res = rend - ybar)
summary(trigo$res)
```

Los residuos de las obervaciones $y_{12}$ y $y_{24}$

```{r}
trigo[c(2, 14), ]
```

##

```{r, fig.height=4}
# gráfico de residuos por grupos
ggplot(trigo, aes(x = Variedad, y = res)) + 
  geom_point() +
  geom_hline(yintercept = 0)
```

En el gráfico se observa que la distribución de los residues dentro de cada tratamiento no sería homogénea.

##

```{r, fig.height=4}
# QQ plot
ggplot(trigo, aes(sample = res)) +
  geom_qq() +
  geom_qq_line()
```

En el gráfico se observa que la distribución de los residuos se aproxima a una distribución normal.


## Prueba homogeneidad varianza

Es un supuesto fuerte que altera los valores de significancia.

$$
H_0: \sigma^2_1 = \sigma^2_2 \\
H_1: \sigma^2_1 \ne \sigma^2_2
$$

**Estadístico de prueba**

$$
F = \dfrac{s^2_1}{s^2_2} \sim F_{n_1-1; n_2-1}
$$

Donde por conveniencia $s^2_1 > s^2_2$

**Regla decisión**

- Si $F > F_{n_1-1; n_2-1; 1-\alpha/2}$, entonces se rechaza $H_0$
- Si $F \le F_{n_1-1; n_2-1; 1-\alpha/2}$, entonces no se rechaza $H_0$


## Ejemplo

Anteriormente asumimos que la varianza de los rendimientos de ambas variedades era la misma.

```{r}
# Prueba igualdad varianza de dos poblaciones
var.test(rend ~ Variedad, trigo)
```

En función de este resultado hay evidencia suficiente para rechazar $H_0$ (p = 0.000408), por lo tanto no se puede asumir que las varianzas son homogéneas, hay que usar la aproximación de correción Welch-Satterwaite.

##

```{r}
# Prueba t para varianzas heterogéneas
t.test(rend ~ Variedad, trigo, var.equal = F)
```

La conclusión final no cambia, las diferencias entre variedades no son estadísticamente significativas al 5% (p = 0.118).

## Muestras apareadas

Se puede disminuir la varianza de los errores $\sigma^2_{e}$ asignando los tratamientos a pares de unidades experimentales homogéneas (_e.g. parcelas próximas, animales mismo peso, edad, sexo, tiempos, etc._)

\biskip

 Par       $y_1$      $y_2$      $y_D = y_1 - y_2$
--------- ---------- ---------- --------------------
 1         $y_{11}$   $y_{21}$    $y_1$              
 2         $y_{12}$   $y_{22}$    $y_2$              
 $\dots$   $\dots$    $\dots$     $\dots$            
 n         $y_{1n}$   $y_{2n}$    $y_n$              
--------- ---------- ---------- --------------------
\bigskip

Luego, la respuesta se mide sobre la diferencia entre pares de observaciones

$$
\bar{y}_D = \dfrac{1}{n_D} \sum (y_{1j} - y_{2j}) \sim N \left(\mu_D, \dfrac{\sigma_D}{\sqrt{n_D}} \right)
$$

## Modelo lineal aditivo | modelo de efectos

$$
y_{ij} = \mu + \tau_i + \rho_j + e_{ij}  \quad \quad i = 1, 2; j = 1, \dots, n_i
$$

donde: 

- $y_{ij}$ es la j-ésima observación del i-ésimo par

- $\mu$ es la media general

- $\tau_i$ es la desviación o efecto del i-ésimo tratamiento respecto de $\mu$, es decir $\tau_i = \mu_i - \mu$

- $\rho_j$ es la desviación o efecto del j-ésimo par.

- $e_{ij}$ es el error aleatorio asociado a la j-ésima observación del i-ésimo tratamiento
modelo lineal

Supuestos: $e_{ij} \sim N(0, \sigma)$

## Prueba hipótesis

$$
H_0: \mu_D = 0 \\
H_1: \mu_D \ne 0 \\
$$

**Estadístico de prueba**

$$
t = \dfrac{\bar{y}_D - \mu_D}{S_{\bar{y}_D}} \sim t_{n_D - 1}
$$

Donde $S_{\bar{y}_D}$ es un estimador muestral del desvió de la diferencia de a pares $\sigma_{\mu_D}$

**Regla decisión**

- Si $| t | > t_{1-\alpha/2,n_D -1}$, entonces se rechaza $H_0$
- Si $| t | \le t_{1-\alpha/2, n_D -1}$, entonces no se rechaza $H_0$


## Ejemplo

Suponiendo que en el ejemplo anterior, el experimentador advertido por las diferencias de suelo, agrupó las parcelas en pares según la similitud de lo suelos. 

```{r, fig.height=3.5}
# Gráfico de pares
ggplot(trigo, aes(x = Variedad, y = rend, color = factor(Parcela))) +
  geom_point() +
  geom_line(aes(group = Parcela))
```

Cada línea conecta los rendimientos de pareclas vecinas. Hay pares donde la diferencia es positiva o negativa.

##

Esto supone que las respuestas entre parcelas próximas estarían correlacionada positivamente.

```{r}
# paquete para manipular datos
library(tidyr)

# Convertir tabla para calculo correlación
trigo2 <- trigo %>% 
  dplyr::select(Parcela, Variedad, rend) %>% 
  spread(., key = Variedad, value = rend)

# Coeficiente de correlacion lineal
dplyr::select(trigo2, Exp1, Exp2) %>% cor()
```

La correlación entre los rendimientos es 0.78. Es decir que las diferencias entre parcelas del mismo par son menores a las diferencias entre parcelas de pares diferentes.

##

En el gráfico de dispersión se puede ver la relación entre los pares de rendimientos

```{r, fig.height=3.5}
# Gráfico dispersión
ggplot(trigo2, aes(x = Exp1, y = Exp2)) +
  geom_point() +
  geom_smooth(method = 'lm', se = F)
```

##

Dado que las observaciones están pareadas por la parcela la variable de interés es la diferencia de rendimientos dentro de cada par. 

```{r, fig.height = 2.5}
# Diferencias de a pares
trigo2 <- mutate(trigo2, dif = Exp1 - Exp2)

ggplot(trigo2, aes(x = "", y = dif)) +
  geom_boxplot() +
  coord_flip()
```

¿Hay diferencias entre tratamientos? ¿En que sentido?

##

La prueba de hipótesis para muestras pareadas.

```{r}
t.test(rend ~ Variedad, trigo, paired = T)
```

**Conclusión**: la muestra no proporciona evidencia suficiente para rechazar $H_0$, por lo tanto las diferencias de rendimiento entre `Exp1` y `Exp2` no son estadísticamente significativas al 5% (p = 0.05868). Con un 95% de confianza, se estima que la verdadera diferencia de rendimiento entre `Exp1` y `Exp2` está contenida en el intervalo -0.20 y 9.06 qq ha^-1^.